{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Tic Tac Toe using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules\n",
    "1. `random`: To generate epsilon (exploration vs exploitation)\n",
    "2. `json`: To store the state space in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class environment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state = ['-' for i in range(9)]\n",
    "        self.symbol_mapping = {'-': 0,'X': 1,'O': 2}\n",
    "        self.state_value = {}\n",
    "    \n",
    "    def display(self):\n",
    "        '''\n",
    "        Displaying the 3x3 grid\n",
    "        '''\n",
    "        for row in range(3):\n",
    "            for col in range(3):\n",
    "                print(\"{}\".format(self.state[3*row+col]), end=' ')\n",
    "            print()\n",
    "            \n",
    "    def state_hash(self, state_matrix):\n",
    "        '''\n",
    "        Calculating the hash value of a state\n",
    "        '''\n",
    "        hash_val = 0\n",
    "        for val in state_matrix:\n",
    "            hash_val = 3*hash_val + self.symbol_mapping[val]\n",
    "        return hash_val\n",
    "    \n",
    "    def list_empty(self):\n",
    "        '''\n",
    "        Finding all the empty spaces in game\n",
    "        '''\n",
    "        empty_index = []\n",
    "        for ind,val in enumerate(self.state):\n",
    "            if val == '-':\n",
    "                empty_index.append(ind)\n",
    "        return empty_index\n",
    "    \n",
    "    def end_game(self):\n",
    "        '''\n",
    "        Returns True/False depending on if there is empty spaces\n",
    "        '''\n",
    "        return len(self.list_empty())==0\n",
    "    \n",
    "    def isWon(self, symbol):\n",
    "        '''\n",
    "        Checks the winning condition in game\n",
    "        '''\n",
    "        for (ind1, ind2, ind3) in [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]:\n",
    "            if(self.state[ind1]==symbol and self.state[ind2]==symbol and self.state[ind3]==symbol):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def update_state_value(self, agent_hash, reward):\n",
    "        '''\n",
    "        Updates the state value according to Reinforcement Learning\n",
    "        '''\n",
    "        gamma = 0.8\n",
    "        for ind,val in enumerate(agent_hash[::-1]):\n",
    "            state_reward,n = self.state_value.setdefault(val, (1,0))\n",
    "            self.state_value[val] = ((state_reward*n + reward)/(n+1), n+1)\n",
    "            reward *= gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - \n",
      "- - - \n",
      "- - - \n",
      "0\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "False\n",
      "False\n",
      "{4: (1.0, 1), 5: (0.8, 1), 6: (0.6400000000000001, 1)}\n"
     ]
    }
   ],
   "source": [
    "e = environment()\n",
    "e.display()\n",
    "print(e.state_hash(e.state))\n",
    "print(e.list_empty())\n",
    "print(e.end_game())\n",
    "print(e.isWon('O'))\n",
    "e.update_state_value([6,5,4],1)\n",
    "print(e.state_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class agent:\n",
    "    \n",
    "    def __init__(self, symbol):\n",
    "        self.symbol = symbol\n",
    "        self.state_hash_stack = []\n",
    "#         self.mapping = {'-': 0, 'X': 1, 'O': 2}\n",
    "    \n",
    "    def action(self, environment):\n",
    "        '''\n",
    "        Determines the action to be taken by the agent\n",
    "        '''\n",
    "        temp_possible_state = []\n",
    "        \n",
    "        # Agent checks for all empty space and gets indexs\n",
    "        for ind in environment.list_empty():\n",
    "            temp_state = environment.state.copy()\n",
    "            temp_state[ind] = self.symbol\n",
    "            \n",
    "            # Find the corresponding hash values\n",
    "            temp_hash_state_value = environment.state_hash(temp_state)\n",
    "            temp_possible_state.append((ind, temp_hash_state_value, environment.state_value.setdefault(temp_hash_state_value, (1,0))))\n",
    "            \n",
    "        # Finds argmax of possiblilty\n",
    "        if random.random() < 0.9:\n",
    "            optimal_action = max(temp_possible_state, key=lambda x: x[2][0])\n",
    "        else:\n",
    "            optimal_action = random.sample(temp_possible_state, 1)[0]\n",
    "        \n",
    "        # Puts in stack\n",
    "        self.state_hash_stack.append(optimal_action[1])\n",
    "        return optimal_action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[6561]\n"
     ]
    }
   ],
   "source": [
    "a = agent('X')\n",
    "print(a.action(e))\n",
    "e.state_value\n",
    "print(a.state_hash_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialising the agent and environment\n",
    "    env = environment()\n",
    "    agentO = agent('O')\n",
    "    agentX = agent('X')\n",
    "    \n",
    "    # 'X' always plays first and t_game is number of games to be played\n",
    "    agent_mapping = {0: agentX, 1: agentO}\n",
    "    chance = 0\n",
    "    t_game = 100_000\n",
    "    \n",
    "    # Iterating for each game\n",
    "    for n_games in range(t_game):\n",
    "        \n",
    "        # Initialising for each game\n",
    "        env.state = ['-' for i in range(9)]\n",
    "        agent_mapping[chance].state_hash_stack = []\n",
    "        agent_mapping[chance^1].state_hash_stack = []\n",
    "        chance = 0\n",
    "                                      \n",
    "        while not env.end_game():\n",
    "            # Calculating and updating the move by agent\n",
    "            move = agent_mapping[chance].action(env)\n",
    "            env.state[move] = agent_mapping[chance].symbol\n",
    "            \n",
    "            # Displaying the last sample game\n",
    "            if n_games == t_game -1:\n",
    "                print(env.state)\n",
    "                env.display()\n",
    "                \n",
    "            # Winning condition, RL updates the state value of environment\n",
    "            if env.isWon(agent_mapping[chance].symbol):\n",
    "                env.update_state_value(agent_mapping[chance].state_hash_stack,1)\n",
    "                env.update_state_value(agent_mapping[chance^1].state_hash_stack,-1)\n",
    "                break\n",
    "                \n",
    "            # Opponent's turn\n",
    "            chance ^= 1\n",
    "            \n",
    "        # If the game is draw, RL updates the state value of environment\n",
    "        if not (env.isWon(agent_mapping[chance].symbol) and env.isWon(agent_mapping[chance^1].symbol)):\n",
    "            env.update_state_value(agent_mapping[chance].state_hash_stack,0)\n",
    "            env.update_state_value(agent_mapping[chance^1].state_hash_stack,0)\n",
    "        \n",
    "        # Displaying the status after every 500 games played\n",
    "        if n_games%500 == 0:\n",
    "            print(\"STATUS: Total games played:{} and length of state space: {}\".format(n_games+500,len(env.state_value)))\n",
    "    \n",
    "    # Storing the result state space in JSON\n",
    "    with open(\"State_space.json\", \"w\") as file:\n",
    "        json.dump(env.state_value, file)\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class User:\n",
    "\n",
    "    def __init__(self, symbol):\n",
    "        self.symbol = symbol\n",
    "        \n",
    "    def action(self, env):\n",
    "        '''\n",
    "        Determines the action to be taken by the user.\n",
    "        Valid input 00, 01, 02, 10, 11, 12, 20, 21, 22\n",
    "        '''\n",
    "        while not env.end_game():\n",
    "            try:\n",
    "                data = int(input(\"Enter position\\n\"))\n",
    "                if data//10 in range(3) and data%10 in range(3):\n",
    "                    index = 3*(data//10) + (data%10)\n",
    "                    if index in env.list_empty():\n",
    "                        return index\n",
    "                    else:\n",
    "                        print(\"Index is taken\")\n",
    "                else:\n",
    "                    print(\"Index out of bound\")\n",
    "                \n",
    "            except:\n",
    "                print(\"Enter 2-digit number [0-2][0-2]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter position\n",
      "13\n",
      "Index out of bound\n",
      "Enter position\n",
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = User('O')\n",
    "user.action(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining trained RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class trained_agent:\n",
    "    \n",
    "    def __init__(self, symbol):\n",
    "        self.symbol = symbol\n",
    "        with open(\"State_space.json\", \"r\") as file:\n",
    "            self.state_value = json.load(file)\n",
    "        print(len(self.state_value))\n",
    "    \n",
    "    def action(self, environment):\n",
    "        '''\n",
    "        Determines the action to be taken by RL trained model\n",
    "        '''\n",
    "        temp_possible_state = []\n",
    "        \n",
    "        # Agent checks for all empty space and gets indexs\n",
    "        for ind in environment.list_empty():\n",
    "            temp_state = environment.state.copy()\n",
    "            temp_state[ind] = self.symbol\n",
    "            \n",
    "            # Find the corresponding hash values\n",
    "            temp_hash_state_value = environment.state_hash(temp_state)\n",
    "            temp_possible_state.append((ind, self.state_value.setdefault(str(temp_hash_state_value), (1,0))))\n",
    "            \n",
    "        # Finds argmax of possiblilty\n",
    "        optimal_action = max(temp_possible_state, key=lambda x: x[1][0])\n",
    "        \n",
    "        return optimal_action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = trained_agent('X')\n",
    "t.action(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User VS RL Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 0 for O and 1 for X1\n",
      "1 <class 'int'>\n",
      "5455\n",
      "Enter position\n",
      "11\n",
      "X move\n",
      "- - - \n",
      "- X - \n",
      "- - - \n",
      "O move\n",
      "- - - \n",
      "- X - \n",
      "- - O \n",
      "Enter position\n",
      "21\n",
      "X move\n",
      "- - - \n",
      "- X - \n",
      "- X O \n",
      "O move\n",
      "- O - \n",
      "- X - \n",
      "- X O \n",
      "Enter position\n",
      "00\n",
      "X move\n",
      "X O - \n",
      "- X - \n",
      "- X O \n",
      "O move\n",
      "X O O \n",
      "- X - \n",
      "- X O \n",
      "Enter position\n",
      "10\n",
      "X move\n",
      "X O O \n",
      "X X - \n",
      "- X O \n",
      "O move\n",
      "X O O \n",
      "X X O \n",
      "- X O \n",
      "O wins\n"
     ]
    }
   ],
   "source": [
    "def user_game():\n",
    "    # Defining the environment\n",
    "    env = environment()\n",
    "    \n",
    "    # Giving the user option to choose 'X' or 'O'\n",
    "    while True:\n",
    "        try:\n",
    "            XO = int(input(\"Enter 0 for O and 1 for X\"))\n",
    "            print(XO, type(XO))\n",
    "            if XO == 0:\n",
    "                user = User('O')\n",
    "                agentX = trained_agent('X')\n",
    "                agent_mapping = {0: agentX, 1: user}\n",
    "                break\n",
    "            elif XO == 1:\n",
    "                user = User('X')\n",
    "                agentO = trained_agent('O')\n",
    "                agent_mapping = {0: user, 1: agentO}\n",
    "                break\n",
    "            else:\n",
    "                print(\"Try again\")\n",
    "        except:\n",
    "            print(\"Wrong input\")\n",
    "    \n",
    "    # 'X' always plays first\n",
    "    chance = 0\n",
    "    \n",
    "    while not env.end_game():\n",
    "        # Finding the action to be played\n",
    "        move = agent_mapping[chance].action(env)\n",
    "        env.state[move] = agent_mapping[chance].symbol\n",
    "        \n",
    "        # Displaying the board state\n",
    "        print(\"{} move\".format(agent_mapping[chance].symbol))\n",
    "        env.display()\n",
    "        \n",
    "        # Winning condition\n",
    "        if env.isWon(agent_mapping[chance].symbol):\n",
    "            print(\"{} wins\".format(agent_mapping[chance].symbol))\n",
    "            break\n",
    "            \n",
    "        # Opponent's turn\n",
    "        chance ^= 1\n",
    "    \n",
    "user_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
